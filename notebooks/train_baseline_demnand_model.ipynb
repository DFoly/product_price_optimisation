{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e30705bd-cf86-44a5-a784-31b2b8df95e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from pathlib import Path\n",
    "# sys.path\n",
    "# import sys\n",
    "# sys.path.append(str(Path.cwd().parent / 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca1a976-59c0-412b-9ef3-07490b2aba76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import pyspark.sql.functions as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from databricks.connect import DatabricksSession\n",
    "from src.pricing.config import ProjectConfig\n",
    "from src.pricing.data_processor import DataProcessor, FeatureProducer\n",
    "from src.pricing.models.baseline_model import LGBMModel\n",
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
    "\n",
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "config = ProjectConfig.from_yaml(config_path=\"../project_config.yml\", env=\"dev\")\n",
    "\n",
    "# Enable automatic reloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace3ea76-0a11-4d52-a84a-53b33c8a77f5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Move to data feature prep class"
    }
   },
   "outputs": [],
   "source": [
    "fe = FeatureEngineeringClient()\n",
    "# data_processor = DataProcessor(config, spark)\n",
    "# df_raw_sales = data_processor.preprocess()\n",
    "\n",
    "# # 2. Instantiate and run the FeatureProducer\n",
    "# feature_producer = FeatureProducer(config, spark)\n",
    "# df_features_to_publish = feature_producer.generate_features_spark(df_raw_sales)\n",
    "\n",
    "# Define the Feature Store table path\n",
    "feature_table_name = f\"{config.catalog_name}.{config.schema_name}.price_features\"\n",
    "training_label_cols = [\"id\", \"date\", \"demand\", \"item_id\", \"store_id\", \"dept_id\", \"cat_id\", \"state_id\", \n",
    "                       \"item_store_id\"] # need this as a lookup\n",
    "\n",
    "training_set_labels = spark.sql(f\"select distinct {','.join(training_label_cols)} from {feature_table_name}\")\n",
    "\n",
    "feature_names = ['wday', 'month', 'year', 'dayofweek', 'dayofyear', 'week', 'snap_CA', 'snap_TX', 'snap_WI', \n",
    "                 'sell_price', 'days_since_first_sale', 'is_event', 'lag_t7', 'rolling_mean_lag7_w7', 'lag_t28', 'rolling_mean_lag28_w7', 'item_running_avg', 'store_running_avg']\n",
    "\n",
    "feature_lookups = [\n",
    "    FeatureLookup(\n",
    "        table_name=feature_table_name,\n",
    "        feature_names=feature_names,\n",
    "        lookup_key=\"item_store_id\",\n",
    "        timestamp_lookup_key=\"date\"\n",
    "    )\n",
    "]\n",
    "\n",
    "training_set = fe.create_training_set(\n",
    "    df=training_set_labels,\n",
    "    feature_lookups=feature_lookups,\n",
    "    #exclude_columns=[\"item_store_id\"],\n",
    "    label=\"demand\",\n",
    ")\n",
    "\n",
    "# Get the final Spark DataFrame for training\n",
    "training_df = training_set.load_df()\n",
    "\n",
    "# sample of data\n",
    "selected_items = [row.item_id for row in training_df.select(\"item_id\").distinct().limit(10).collect()]\n",
    "selected_stores = [row.store_id for row in training_df.select(\"store_id\").distinct().limit(10).collect()]\n",
    "training_df = training_df.filter(\n",
    "    (F.col(\"item_id\").isin(selected_items)) &\n",
    "    (F.col(\"store_id\").isin(selected_stores))\n",
    ")\n",
    "\n",
    "# Run the cross-validation loop on the final training DataFrame\n",
    "training_df_pandas = training_df.toPandas()\n",
    "training_df_pandas.dropna(inplace=True)\n",
    "\n",
    "# label encoding for categorical data\n",
    "le = LabelEncoder()\n",
    "categorical_cols = ['item_id', 'store_id', 'dept_id', 'cat_id', 'state_id']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    # Check if the column exists to prevent errors\n",
    "    if col in training_df_pandas.columns:\n",
    "        # Fit and transform the data in place\n",
    "        training_df_pandas[col] = le.fit_transform(training_df_pandas[col])\n",
    "\n",
    "training_df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a8af78-00a4-4edd-9334-98114317de38",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DEMAND MODEL"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate and train the model\n",
    "lgbm_params = {\n",
    "    'objective': 'poisson',\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 2_000,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "demand_model = LGBMModel(spark=spark, config=config, training_df=training_df_pandas, params=lgbm_params, model_type='forecast')\n",
    "demand_model.train_with_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e55d88-2519-46a9-ae03-6c5ac63b0970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "demand_model.log_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50afd7dd-4e3f-4ad5-9c01-f963484b994f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a9f9eba-19b4-4bad-8a95-a209ee4cd479",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ELASTICITY MODEL"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "lightgbm",
     "databricks-feature-engineering"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8995768872484635,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "train_baseline_demnand_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
