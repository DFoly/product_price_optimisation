{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e30705bd-cf86-44a5-a784-31b2b8df95e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from pathlib import Path\n",
    "# sys.path\n",
    "# import sys\n",
    "# sys.path.append(str(Path.cwd().parent / 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca1a976-59c0-412b-9ef3-07490b2aba76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import lightgbm as lgb\n",
    "import pyspark.sql.functions as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from databricks.connect import DatabricksSession\n",
    "from src.pricing.config import ProjectConfig\n",
    "from src.pricing.data_processor import DataProcessor, FeatureProducer\n",
    "from src.pricing.models.baseline_model import LGBMModel\n",
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
    "\n",
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "config = ProjectConfig.from_yaml(config_path=\"../project_config.yml\", env=\"dev\")\n",
    "\n",
    "# Enable automatic reloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace3ea76-0a11-4d52-a84a-53b33c8a77f5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Move to data feature prep class"
    }
   },
   "outputs": [],
   "source": [
    "fe = FeatureEngineeringClient()\n",
    "# data_processor = DataProcessor(config, spark)\n",
    "# df_raw_sales = data_processor.preprocess()\n",
    "\n",
    "# # 2. Instantiate and run the FeatureProducer\n",
    "# feature_producer = FeatureProducer(config, spark)\n",
    "# df_features_to_publish = feature_producer.generate_features_spark(df_raw_sales)\n",
    "\n",
    "# Define the Feature Store table path\n",
    "feature_table_name = f\"{config.catalog_name}.{config.schema_name}.price_features\"\n",
    "training_label_cols = [\"id\", \"date\", \"demand\", \"item_id\", \"store_id\", \"dept_id\", \"cat_id\", \"state_id\"]\n",
    "\n",
    "training_set_labels = spark.sql(f\"select distinct {','.join(training_label_cols)} from {feature_table_name}\")\n",
    "\n",
    "feature_names = ['wday', 'month', 'year', 'dayofweek', 'dayofyear', 'week', 'snap_CA', 'snap_TX', 'snap_WI', \n",
    "                 'sell_price', 'days_since_first_sale', 'is_event', 'lag_t7', 'rolling_mean_lag7_w7', 'lag_t28', 'rolling_mean_lag28_w7', 'item_running_avg', 'store_running_avg']\n",
    "\n",
    "feature_lookups = [\n",
    "    FeatureLookup(\n",
    "        table_name=feature_table_name,\n",
    "        feature_names=feature_names,\n",
    "        lookup_key=\"id\",\n",
    "        timestamp_lookup_key=\"date\"\n",
    "    )\n",
    "]\n",
    "\n",
    "training_set = fe.create_training_set(\n",
    "    df=training_set_labels,\n",
    "    feature_lookups=feature_lookups,\n",
    "    #exclude_columns=[\"item_store_id\"],\n",
    "    label=\"demand\",\n",
    ")\n",
    "\n",
    "# Get the final Spark DataFrame for training\n",
    "training_df = training_set.load_df()\n",
    "\n",
    "##############################\n",
    "# sample of data for testing\n",
    "##############################\n",
    "selected_items = [row.item_id for row in training_df.select(\"item_id\").distinct().limit(10).collect()]\n",
    "selected_stores = [row.store_id for row in training_df.select(\"store_id\").distinct().limit(10).collect()]\n",
    "training_df = training_df.filter(\n",
    "    (F.col(\"item_id\").isin(selected_items)) &\n",
    "    (F.col(\"store_id\").isin(selected_stores))\n",
    ")\n",
    "\n",
    "# Run the cross-validation loop on the final training DataFrame\n",
    "training_df_pandas = training_df.toPandas()\n",
    "training_df_pandas.dropna(inplace=True)\n",
    "\n",
    "# label encoding for categorical data\n",
    "le = LabelEncoder()\n",
    "categorical_cols = ['item_id', 'store_id', 'dept_id', 'cat_id', 'state_id']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    # Check if the column exists to prevent errors\n",
    "    if col in training_df_pandas.columns:\n",
    "        # Fit and transform the data in place\n",
    "        training_df_pandas[col] = le.fit_transform(training_df_pandas[col])\n",
    "\n",
    "training_df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a8af78-00a4-4edd-9334-98114317de38",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DEMAND MODEL"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate and train the model\n",
    "lgbm_params = {\n",
    "    'objective': 'poisson',\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 2_000,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "demand_model = LGBMModel(spark=spark, config=config, training_df=training_df_pandas.copy(deep=True), params=lgbm_params, model_type='forecast', baseline_forecasts_daily_dict = {})\n",
    "demand_model.train_with_cv()\n",
    "demand_model.log_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdbca76c-89df-4603-9344-ff2a997e24c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lgb.plot_importance(demand_model.ensemble_models[0], importance_type=\"gain\", precision=0, height=0.5, figsize=(6, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "306ccdec-a26a-4f05-a902-9bf6b59bc9b8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Model Predictions"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "logged_model = 'runs:/71ce2a4b05de4994a69939c26e8104bf/lightgbm-ensemble-model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# should make baseline predictions based on training data so we can pass to the elasticity model\n",
    "baseline_features_to_predict = demand_model.X_train[demand_model.features]\n",
    "baseline_predictions_array = loaded_model.predict(baseline_features_to_predict)\n",
    "\n",
    "baseline_forecasts_daily_dict = {}\n",
    "for i, (idx, row) in enumerate(demand_model.X_train.iterrows()):\n",
    "    item_id = row['item_id']\n",
    "    store_id = row['store_id']\n",
    "    date_string = row['date'].strftime('%Y-%m-%d')\n",
    "    prediction = baseline_predictions_array[i]\n",
    "    \n",
    "    key = (item_id, store_id, date_string)\n",
    "    baseline_forecasts_daily_dict[key] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e55d88-2519-46a9-ae03-6c5ac63b0970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# logged_model = mlflow.get_logged_model(demand_model.model_info.model_id)\n",
    "# model = mlflow.sklearn.load_model(f\"models:/{demand_model.model_info.model_id}\")\n",
    "\n",
    "# run_id = mlflow.search_runs(\n",
    "#     experiment_names=[\"/Shared/marvel-characters-basic\"]\n",
    "# ).run_id[0]\n",
    "\n",
    "# model = mlflow.sklearn.load_model(f\"runs:/{run_id}/lightgbm-pipeline-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a9f9eba-19b4-4bad-8a95-a209ee4cd479",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ELASTICITY MODEL"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate and train the model\n",
    "lgbm_params = {\n",
    "    'objective': 'poisson',\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 2_000,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "elasticity_model = LGBMModel(spark=spark, config=config, training_df=training_df_pandas.copy(deep=True), params=lgbm_params, model_type='elasticity', baseline_forecasts_daily_dict=baseline_forecasts_daily_dict)\n",
    "elasticity_model.train_with_cv()\n",
    "elasticity_model.log_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fc53987-e080-4ab3-a6cf-879850e86d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_df_pandas['date'].max(), demand_model.training_df_pandas['date'].max(), elasticity_model.training_df_pandas['date'].max(), demand_model.split_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e404dae4-022d-4937-bf7a-2235e9559c21",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feed Predictions into Elasticity Model"
    }
   },
   "outputs": [],
   "source": [
    "temp = elasticity_model.training_df_pandas\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96e0c6fe-5725-4ed0-93bd-f334c72a8366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp = elasticity_model.training_df_pandas.query('date >= @elasticity_model.split_date')\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9caffa50-9ac6-4510-b62c-13fa7f7a7b0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "elasticity_model.split_date, elasticity_model.training_df_pandas['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34f09390-3f1a-4da3-9f38-307afcfd5ec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "lightgbm",
     "databricks-feature-engineering",
     "loguru"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8315547017975091,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "train_baseline_demand_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
